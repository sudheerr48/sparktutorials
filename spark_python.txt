pyspark --master yarn --conf spark.ui.port=12889

-------------------------------------------------------------------------------

Creating RDD

orderItems = sc.textFile("/public/retail_db/order_items")
type(orderItems)
help(orderItems)
orderItems.first()
for i in orderItems.take(10): print(i)


l = range(1, 10000)
lRDD = sc.parallelize(l)
productsRaw = open("/data/retail_db/products/part-00000").read().splitlines()
type(productsRaw)
productsRDD = sc.parallelize(productsRaw)
type(productsRDD)
productsRDD.first()
for i in productsRDD.take(10): print(i)
productsRDD.count()

SQLContext have 2 APIs to read data of different file formats

load – typically takes 2 arguments, path and format
read – have interface for each of the file format (e.g.: read.json)
Following are the file formats supported

text
orc
parquet
json (example shown)
csv (3rd party plugin)
avro (3rd party plugin, but Cloudera clusters get by default)


sqlContext.load("/public/retail_db_json/order_items", "json").show()
sqlContext.read.json("/public/retail_db_json/order_items").show()


------------------------------------------------------

Row level transformations

#String Manipulation
orders = sc.textFile("/public/retail_db/orders")
s = orders.first()

#first character from a string
s[0]

#first 10 characters from a string
s[:10]

#get length of string
len(s)

#One way to get the date, but it will not work if the order id before first
#comma is more than one character or digit
s[2:12]

#split and extract date
s.split(",")
type(s.split(","))
#Get Date
s.split(",")[1]
#Get customer id
s.split(",")[2]

#type casting to integer
int(s.split(",")[0])

#type casting integer to string
print("printing " + str(1))

int(s.split(",")[1].split(" ")[0].replace("-", ""))

#map
orders = sc.textFile("/public/retail_db/orders")
help(orders.map)

#Get status
orders.map(lambda o: o.split(",")[3]).first()
#Get count
orders.map(lambda o: o.split(",")[1]).first()

#Convert date format from YYYY-MM-DD HH24:MI:SS -> YYYYMM
#Type cast date to integer
orders.map(lambda o: int(o.split(",")[1].split(" ")[0].replace("-", ""))).first()
orders.map(lambda o: int(o.split(",")[1].split(" ")[0].replace("-", ""))).take(10)
orders.map(lambda o: int(o.split(",")[1].split(" ")[0].replace("-", ""))).count()

#Create tuples
orders.map(lambda o: (o.split(",")[3], 1))

orderItems = sc.textFile("/public/retail_db/order_items")
orderItems.first()
for i in orderItems.take(10): print(i)
orderItemsMap = orderItems. \
map(lambda oi: (int(oi.split(",")[1]), float(oi.split(",")[4])))
orderItemsMap.first()
for i in orderItemsMap.take(10): print(i)



#flatMap
linesList = ["How are you", "let us perform", "word count using flatMap", "to understand flatMap in detail"]
lines = sc.parallelize(linesList)
words = lines.flatMap(lambda l: l.split(" "))
for i in words.collect():print(i)

tuples = words.map(lambda word: (word, 1))
for i in tuples.countByKey(): print(i)

w= lines.map(lambda l: l.split(" "))
for i in w.collect():print(i)


------------------------------------------------------------------------
Filtering the data


orders = sc.textFile("/public/retail_db/orders")
ordersComplete = orders. \
filter(lambda o: 
  o.split(",")[3] in ["COMPLETE", "CLOSED"] and o.split(",")[1][:7] == "2014-01")

for i in ordersComplete.take(10):print(i)


or I can also write as 

ordersComplete = orders.filter(lambda o : (o.split(",")[3] == 'COMPLETE' or o.split(",")[3] == 'CLOSED') and o.split(",")[1][:7] == "2014-01" )

for i in ordersComplete.take(10):print(i)


------------------------------------------------------------------
JOINS

orders = sc.textFile("/public/retail_db/orders")
orderitems = sc.textFile("/public/retail_db/order_items")

for i in orders.take(10):print(i)
for i in orderitems.take(10):print(i)

om = orders.map(lambda i : (int(i.split(",")[0]),i.split(",")[1]))
for i in om.take(10):print(i)

oim = orderitems.map(lambda i : (int(i.split(",")[1]),float(i.split(",")[4])))
for i in oim.take(10):print(i)

ojoin = om.join(oim)
for i in ojoin.take(10):print(i)


OUTERJOIN

1.)

OLOJoin = om.leftOuterJoin(oim)
for i in OLOJoin.take(100):print(i)

2.)

om = orders.map(lambda i : (int(i.split(",")[0]),i.split(",")[3]))
for i in om.take(10):print(i)

oim = orderitems.map(lambda i : (int(i.split(",")[1]),float(i.split(",")[4])))
for i in oim.take(10):print(i)


OLOJoin = om.leftOuterJoin(oim)
for i in OLOJoin.take(100):print(i)

OLOJoin.count()
ojoin.count()

only None

oon = OLOJoin.filter(lambda i : i[1][1] == None)
for i in oon.take(10):print(i)

oon.count()



Right outer join with right code

#outer join
orders = sc.textFile("/public/retail_db/orders")
orderItems = sc.textFile("/public/retail_db/order_items")

ordersMap = orders. \
map(lambda o:(int(o.split(",")[0]), o.split(",")[3]))

orderItemsMap = orderItems. \
map(lambda oi:(int(oi.split(",")[1]), float(oi.split(",")[4])))

ordersLeftOuterJoin = ordersMap.leftOuterJoin(orderItemsMap)

ordersLeftOuterJoinFilter = ordersLeftOuterJoin. \
filter(lambda o: o[1][1] == None)

for i in ordersLeftOuterJoin.take(10): print(i)

ordersRightOuterJoin = orderItemsMap.rightOuterJoin(ordersMap)
for i in ordersRightOuterJoin.take(100): print(i)

ordersRightOuterJoinFilter = ordersRightOuterJoin. \
filter(lambda o: o[1][0] == None)

for i in ordersRightOuterJoinFilter.take(10): print(i)

--------------------------------------------------------------------



#Aggregations - total
orderItems = sc.textFile("/public/retail_db/order_items")
orderItems.count()

#Aggregations - total - Get revenue for given order_id
orderItems = sc.textFile("/public/retail_db/order_items")
orderItemsFiltered = orderItems. \
filter(lambda oi: int(oi.split(",")[1]) == 2)
orderItemsSubtotals = orderItemsFiltered. \
map(lambda oi: float(oi.split(",")[4]))

from operator import add
# orderItemsSubtotals.reduce(add)
orderItemsSubtotals.reduce(lambda x, y: x + y)

--------------------------------------------------------------------

Aggregations by key

1.)countByKey

orders = sc.textFile("/public/retail_db/orders")
for i in orders.take(10):print(i)

orderstatus = orders.map(lambda o: (o.split(",")[3],1))
for i in orderstatus.take(10):print(i)

ans = orderstatus.countByKey()
#Note here and is a dictionary not RDD since count by key is an action
ans


2.GroupByKey()



for i in orderImap.take(10):print(i)


oigroupbyid = orderImap.groupByKey()
for i in oigroupbyid.take(10):print(i)

#Output is in format
#(2, <pyspark.resultiterable.ResultIterable object at 0x5f790d0>)
#(4, <pyspark.resultiterable.ResultIterable object at 0x5f77e50>)
#(8, <pyspark.resultiterable.ResultIterable object at 0x5f77e10>)

l = oigroupbyid.first()
l[0] -> 2
l[1] -> <pyspark.resultiterable.ResultIterable object at 0x5f790d0>
list(l[1]) -> [199.0,255.0,129.00]

list(l[1])[0] -> 199.0
sum(l[1]) -> 579.8

revenueperorder = oigroupbyid.map(lambda i: (i[0],sum(i[1])))
for i in revenueperorder.take(10):print(i)

for rounding off to near integer

revenueperorder = oigroupbyid.map(lambda i: (i[0],round(sum(i[1]))))
for i in revenueperorder.take(10):print(i)


Sorting using groupByKey

here i have to create a rdd in for (orderId , entire list) then apply groupByKey to it and apply sorted to the list with revenue as key


orderItems = sc.textFile("/public/retail_db/order_items")
for i in orderItems.take(10):print(i)

#create rdd in the form of (order_id,revenue)


orderImap = orderItems.map(lambda o: (int(o.split(",")[1]),o))
for i in orderImap.take(10):print(i)

#not working 


#orderItemsSortedbySubtotalPerOrder = orderImap.flatMap(lambda oi:sorted(oi[1],key = lambda k:float(k.split(",")[4]),reverse=True))

#for i in orderItemsSortedbySubtotalPerOrder.take(10):print(i)




